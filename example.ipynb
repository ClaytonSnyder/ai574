{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Notebook\n",
    "\n",
    "1. Follow the \"GETTING STARTED\" steps laid out in the README first\n",
    "1. Click \"Select Kernel\" in upper right corner of this pane\n",
    "1. Click \"Python Environments...\"\n",
    "1. Click \".venv (Python 3.12.3)\"\n",
    "1. Click \"Run All\"\n",
    "1. When prompted if you want to install ipykernel, click \"Install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we create a project\n",
    "\n",
    "Projects house all of the data for an experiment including all of the raw datasources, model, weights/biases, context (questions/answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:31:47:INFO:Deleting project: green_ggs...\n",
      "2024-06-18 23:31:47:INFO:Project deleted.\n",
      "2024-06-18 23:31:47:INFO:Creating new project: green_ggs...\n",
      "2024-06-18 23:31:47:INFO:Project Created.\n"
     ]
    }
   ],
   "source": [
    "from fy_bot.project import create_project, delete_project, project_exists\n",
    "\n",
    "PROJECT_NAME = \"tax_examples\"\n",
    "\n",
    "# This notebook will start from scratch\n",
    "# So, if the project already exists delete it\n",
    "# and create a new project\n",
    "if project_exists(PROJECT_NAME):\n",
    "    delete_project(PROJECT_NAME)\n",
    "\n",
    "create_project(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we add data sources to the project\n",
    "\n",
    "We use add_document with a url to download the document. Once the document is downloaded it is placed in the downloads folder of the project. The document is then scraped for raw text. The raw text is saved in the raw folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 23:31:53:INFO:Adding document https://www.site.uottawa.ca/~lucia/courses/2131-02/A2/trythemsource.txt to project green_ggs\n",
      "2024-06-18 23:31:54:INFO:File downloaded successfully...\n",
      "2024-06-18 23:31:54:INFO:Raw text successfully extracted.\n"
     ]
    }
   ],
   "source": [
    "from fy_bot.datasource import add_document, compile_corpus\n",
    "\n",
    "# Adding documents to the project downloads this file\n",
    "# and puts it in the \"downloads\" folder of the project\n",
    "# It then extracts the raw text from the downloaded file\n",
    "# and saves it in the \"raw\" folder of the project\n",
    "\n",
    "# Tax publications\n",
    "# https://www.irs.gov/forms-pubs/ebook\n",
    "\n",
    "# Your Federal Income Tax (For Individuals)\n",
    "add_document(PROJECT_NAME, \"https://www.irs.gov/pub/ebook/p17.epub\")\n",
    "\n",
    "# Federal Income Tax Withholding Methods\n",
    "add_document(PROJECT_NAME, \"https://www.irs.gov/pub/ebook/p15t.epub\")\n",
    "\n",
    "# Armed Forces' Tax Guide\n",
    "add_document(PROJECT_NAME, \"https://www.irs.gov/pub/ebook/p3.epub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the corpus\n",
    "\n",
    "Since you can add multiple documents to a project we must compile the corpus. During compilation all raw texts are aggregated and cleaned.During the cleaning all sentences that are syntactically invalid are removed. We do this cleaning because we are possibly scraping pdfs/ebooks/transcripts/etc so there will be a bunch of non-sensical text that was scraped. We discard that and we are left with only syntactically valid sentences. The logic for this cleaning can be found in the __is_syntactically_correct method in fy_bot/datasource.py. Additional cleaning is done by converting all characters to lowercase, special characters are removed, useless whitespace is removed, etc. The logic for this cleaning can be found in the compile_corpus method in fy_bot/datasource.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\clayt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-06-18 23:31:54:INFO:Compiling corpus...\n",
      "Cleaning text..: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Writing corpus to disk..: 100%|██████████| 74/74 [00:00<00:00, 73846.89it/s]\n",
      "2024-06-18 23:31:55:INFO:Corpus compilation complete.\n"
     ]
    }
   ],
   "source": [
    "# Compiling the corpous cleans all the raw data files and\n",
    "# saves the aggregated file to corpus.txt in the project folder.\n",
    "compile_corpus(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create context\n",
    "\n",
    "To create a chatbot we need to train our model with context (i.e., question/answer dialog). Since all of our datasources we must generate this context as a preprocessing step. Currently, we do this using a pretrained T5 model (https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap). We can explore other options, but for now, it does the job. The logic for the context generation can by found in fy_bot/content_generation.py. Ass part of this process three files are created: context.txt, questions.txt, answers.txt. context.txt is the context in question/answer format. answers.txt is just the answers. questions.txt is just the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\clayt\\Documents\\gradschoold\\ai574\\project\\ai574\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-18 23:31:57:INFO:Compiling corpus...\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Generating context questions...:   0%|          | 0/74 [00:00<?, ?it/s]c:\\Users\\clayt\\Documents\\gradschoold\\ai574\\project\\ai574\\.venv\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:290: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "Generating context questions...: 100%|██████████| 74/74 [00:25<00:00,  2.87it/s]\n",
      "Writing context...: 100%|██████████| 50/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'What do i not like about samiam?': 'i do not like that samiam!', 'Would you like green eggs and ham?': 'do would you like green eggs and ham?', 'Do you like themsamiam?': 'i do not like themsamiam.', 'What do i not like about green eggs and ham?': 'i do not like green eggs and ham!', 'Would you like them here or there?': 'would you like them here or there?', 'i would not like them here or there.': 'i would not like them here or there.', 'Where would i find them?': 'i would not like them anywhere.', \"What do you think about samiam's comments?\": 'i do not like them samiam.', 'Would you like them in a house?': 'would you like them in a house?', 'What would you like to do with a mouse?': 'would you like then with a mouse?', 'Do i like them in a house?': 'i do not like them in a house.', 'What do i not like with a mouse?': 'i do not like them with a mouse.', 'Do i like them here or there?': 'i do not like them here or there.', 'What do i not like about them?': 'i do not like them anywhere.', 'Would you eat them in a box?': 'would you eat them in a box?', 'Would you eat them with a fox?': 'would you eat them with a fox?', 'I would not eat them here or there.': 'i would not eat them here or there.', 'I would not eat them anywhere.': 'i would not eat them anywhere.', 'What would i not eat green eggs and ham?': 'i would not eat green eggs and ham.', 'What do you think about these books?': 'you may like them.', 'What will you see?': 'you will see.', 'You may like them in a tree?': 'you may like them in a tree!', 'What do you let me be?': 'you let me be!', 'Why do i not like them in a box?': 'i do not like them in a box.', 'Do i like foxes with a fox?': 'i do not like them with a fox.', 'What kind of person can you be?': 'let me be!', 'Will i eat them in a house?': 'i will not eat them in a house.', 'i will not eat them here or there.': 'i will not eat them here or there.', 'I will not eat them anywhere.': 'i will not eat them anywhere.', 'Do i eat greem eggs or ham?': 'i do not eat greem eggs and ham.', \"i don't like them sam you see?\": 'i do not like them sam you see.', \"i don't like them anywhere!\": 'i do not like them anywhere!', 'Do you like green eggs and ham?': 'you do not like green eggs and ham?', 'I will not eat them in the rain.': 'i will not eat them in the rain.', 'Do you like them?': 'you do not like them.', 'What do you say?': 'so you say.', 'What can you do with them?': 'try them and you may i say.', 'If you let me be, i will try them.': 'if you let me be i will try them.', 'What does he try?': '... and he tries them ... say!', 'What kind of food do i like?': 'i do so like green eggs and ham!', 'What do you think?': 'i do!', \"What do you think about samiam's posts?\": 'i like them samiam!', 'What would i eat in a boat?': 'and i would eat them in a boat.', 'What will i do with the leftovers?': 'so i will eat them in a box.', 'What will i eat with a fox?': 'and i will eat them with a fox.', 'I will eat them in a house?': 'and i will eat them in a house.', 'What will i eat with a mouse?': 'and i will eat them with a mouse.', 'What do i eat?': 'and i will eat them here and there.', 'Where can i eat them?': 'i will eat them anywhere!', 'How do you feel about me?': 'thank you sam i am.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from fy_bot.context_generation import generate_context_question\n",
    "\n",
    "questions = generate_context_question(PROJECT_NAME, device)\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from fy_bot.model import build_model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fy_bot.model import get_tokenized_context\n",
    "\n",
    "question_encodings, answer_encodings = get_tokenized_context(PROJECT_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
